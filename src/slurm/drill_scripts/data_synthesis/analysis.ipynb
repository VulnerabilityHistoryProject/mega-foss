{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The following code analyzes the viable_patches_json file. The points of analysis are described below. The primary tool for this analysis is pydriller.\n",
    "\n",
    "1. Total size of the cloned repos\n",
    "2. Total number of vulnerability inducing commits (vuln commits) found & not found\n",
    "3. Average number of months between vuln commit and patch commit (or fix)\n",
    "4. Average number of commits between the vuln commit & patch commit (or fix)\n",
    "5. Average number of vuln commits fixed by patch commit (or fix)\n",
    "6. Percentage of vulns where the vuln commit and fix were made by the same person\n",
    "\n",
    "\n",
    "##### Sources\n",
    "- @inbook{PyDriller,\n",
    "    title = \"PyDriller: Python Framework for Mining Software Repositories\",\n",
    "    abstract = \"Software repositories contain historical and valuable information about the overall development of software systems. Mining software repositories (MSR) is nowadays considered one of the most interesting growing fields within software engineering. MSR focuses on extracting and analyzing data available in software repositories to uncover interesting, useful, and actionable information about the system. Even though MSR plays an important role in software engineering research, few tools have been created and made public to support developers in extracting information from Git repository. In this paper, we present PyDriller, a Python Framework that eases the process of mining Git. We compare our tool against the state-of-the-art Python Framework GitPython, demonstrating that PyDriller can achieve the same results with, on average, 50% less LOC and significantly lower complexity.URL: https://github.com/ishepard/pydrillerMaterials: https://doi.org/10.5281/zenodo.1327363Pre-print: https://doi.org/10.5281/zenodo.1327411\",\n",
    "    author = \"Spadini, Davide and Aniche, MaurÃ­cio and Bacchelli, Alberto\",\n",
    "    year = \"2018\",\n",
    "    doi = \"10.1145/3236024.3264598\",\n",
    "    booktitle = \"The 26th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE)\",\n",
    "    }\n",
    "\n",
    "##### Author @Trust-Worthy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading in the results from the patch_vuln_match.json file and processing objects according to JSONL standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           cve_id                     repo  \\\n",
      "0   CVE-1999-0199             bminor/glibc   \n",
      "1   CVE-1999-0731         KDE/kde1-kdebase   \n",
      "2   CVE-2002-2443                krb5/krb5   \n",
      "3  CVE-2005-10002  wp-plugins/secure-files   \n",
      "4  CVE-2005-10003      mikexstudios/xcomic   \n",
      "\n",
      "                               patch_commit  \\\n",
      "0  2864e767053317538feafa815046fff89e5a16be   \n",
      "1  04906bd5de2f220bf100b605dad37b4a1d9a91a6   \n",
      "2  cf1a0c411b2668c57c41e9c4efd15ba17b6b322c   \n",
      "3  cab025e5fc2bcdad8032d833ebc38e6bd2a13c92   \n",
      "4  6ed8e3cc336e29f09c7e791863d0559939da98bf   \n",
      "\n",
      "                                        vuln_commits  \\\n",
      "0  [dc5efe83c0252ad45337ab98eff6c26fdb29b0a9, 27a...   \n",
      "1                                                 []   \n",
      "2  [e88f857c3680ea395c0bed6a82862d8ea1177221, 438...   \n",
      "3         [b1afc063fd49cfb875e1c6f591543ebff6649469]   \n",
      "4                                                 []   \n",
      "\n",
      "                                          vuln_files  \n",
      "0  [elf/dl-load.c, manual/search.texi, misc/syslo...  \n",
      "1                                                 []  \n",
      "2                        [src/kadmin/server/schpw.c]  \n",
      "3                                 [secure-files.php]  \n",
      "4                                                 []  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jsonlines    \n",
    "\n",
    "json_path:str = \"../production_ready/patch_vuln_match.jsonl\"\n",
    "\n",
    "data: list[object] = []\n",
    "\n",
    "with jsonlines.open(json_path) as reader:\n",
    "\n",
    "    data = [entry for entry in reader]\n",
    "\n",
    "# Convert the list of dictionaries into a pandas DataFrame\n",
    "patch_vuln_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Define a function to extract the file paths and commits\n",
    "def extract_vuln_files_commits(vuln_commits):\n",
    "    if vuln_commits:\n",
    "        files = list(vuln_commits.keys())\n",
    "        commits = [commit for commits in vuln_commits.values() for commit in commits]\n",
    "        return pd.Series([files, commits])\n",
    "    else:\n",
    "        return pd.Series([[], []])  # Empty lists if no vuln_commits\n",
    "\n",
    "# Apply the function to create new columns\n",
    "patch_vuln_df[['vuln_files', 'vuln_commits']] = patch_vuln_df['vuln_commits'].apply(extract_vuln_files_commits)\n",
    "\n",
    "\n",
    "\n",
    "print(patch_vuln_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is where the fun begins.... (iykyk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport shutil\\nfrom pydriller import Repository, Commit\\n\\n# Calculate repo size\\ndef get_directory_size(path: str, total_size: float):\\n    for dirpath, _, filenames in os.walk(path):\\n        for f in filenames:\\n            fp = os.path.join(dirpath, f)\\n            total_size += os.path.getsize(fp)\\n    return total_size\\n\\n\\ncount = 1\\nSIZE_OF_ALL_CLONED_REPOS: list[float] = 0 ### size in MB\\nTOTAL_NUM_MONTHS: int = 0\\nTOTAL_PATCH_VULN_PAIRS: int = 0\\nTOTAL_NUM_COMMITS: int = 0\\n\\nTOTAL_VULNS: int = 0\\n\\n### Averages --> my goal!\\nAVERAGE_NUM_MONTHS_BETWEEN_VULN_N_PATCH: float = 0.0\\nAVERAGE_NUM_COMMITS_BETWEEN_VULN_N_PATCH: float = 0.0\\n\\n\\n\\n### Point 1, 3, 4 , 6\\nfor owner_repo, patch_commit, vuln_commits in zip(\\n    patch_vuln_df[\"repo\"], \\n    patch_vuln_df[\"patch_commit\"], \\n    patch_vuln_df[\"vuln_commits\"]\\n):\\n    print(\"Working on iteration --{count}-- of df)\\n\\n    # Compose remote repo for pydriller\\n    owner, repo = owner_repo.split(\"/\")\\n    remote_url: str = f\"https://github.com/{ownner}/{repo}.git\"\\n\\n    TOTAL_VULNS += len(vuln_commits)\\n\\n    # Create of list of files to pass into Repository object \\n    commits_to_analyze: list[str] = []\\n    commits_to_analyze.append(patch_commit)\\n    commits_to_analyze.extend(vuln_comits)\\n\\n\\n    \\n\\n    # PyDriller clones the repo to a temporary directory\\n    temp_repo: Repository = Repository(remote_url,commits_to_analyze)\\n\\n    \\n    for commit in temp_repo.traverse_commits():\\n        temp_repo_path = commit.project_path  # Path to the cloned repo\\n        \\n        repo_size: float = get_directory_size(temp_repo_path, SIZE_OF_CLONED_REPOS) / (1024 * 1024)  # Convert to MB\\n        \\n        SIZE_OF_ALL_CLONED_REPOS += repo_size\\n    \\n        \\n        \\n        \\n        shutil.rmtree(temp_repo_path)\\n\\n    count+=1\\n\\nAVERAGE_NUM_MONTHS_BETWEEN_VULN_N_PATCH = (TOTAL_NUM_MONTHS / TOTAL_PATCH_VULN_PAIRS )\\n\\n\\n### Point 2,Point 5\\nTOTAL_PATCH_COMMITS_w_VULN_COMMIT: int = 0\\nTOTAL_VULN_COMMITS_FOUND: int = 0\\nTOTAL_NOT_FOUND: int = 0\\n    \\n\\nAVERAGE_NUM_OF_VULNS_TO_PATCH: float = (TOTAL_VULN_COMMITS_FOUND / TAL_PATCH_COMMITS_w_VULN_COMMIT)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Obtaining ...\n",
    "\n",
    "1. Total size of the cloned repos\n",
    "2. Total number of vulnerability inducing commits (vuln commits) found & not found\n",
    "3. Average number of months between vuln commit and patch commit (or fix)\n",
    "4. Average number of commits between the vuln commit & patch commit (or fix)\n",
    "5. Average number of vuln commits fixed by patch commit (or fix)\n",
    "6. Percentage of vulns where the vuln commit and fix were made by the same person\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "'''\n",
    "import os\n",
    "import shutil\n",
    "from pydriller import Repository, Commit\n",
    "\n",
    "# Calculate repo size\n",
    "def get_directory_size(path: str, total_size: float):\n",
    "    for dirpath, _, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n",
    "\n",
    "count = 1\n",
    "SIZE_OF_ALL_CLONED_REPOS: list[float] = 0 ### size in MB\n",
    "TOTAL_NUM_MONTHS: int = 0\n",
    "TOTAL_PATCH_VULN_PAIRS: int = 0\n",
    "TOTAL_NUM_COMMITS: int = 0\n",
    "\n",
    "TOTAL_VULNS: int = 0\n",
    "BY_SAME_PERSON: int = 0\n",
    "PERCENTAGE_OF_VULN_N_PATCH_BY_SAME_PERSON: float = 0.0\n",
    "\n",
    "### Averages --> my goal!\n",
    "AVERAGE_NUM_MONTHS_BETWEEN_VULN_N_PATCH: float = 0.0\n",
    "AVERAGE_NUM_COMMITS_BETWEEN_VULN_N_PATCH: float = 0.0\n",
    "\n",
    "\n",
    "\n",
    "### Point 1, 3, 4 , 6\n",
    "for owner_repo, patch_commit, vuln_commits in zip(\n",
    "    patch_vuln_df[\"repo\"], \n",
    "    patch_vuln_df[\"patch_commit\"], \n",
    "    patch_vuln_df[\"vuln_commits\"]\n",
    "):\n",
    "    print(\"Working on iteration --{count}-- of df)\n",
    "\n",
    "    # Compose remote repo for pydriller\n",
    "    owner, repo = owner_repo.split(\"/\")\n",
    "    remote_url: str = f\"https://github.com/{ownner}/{repo}.git\"\n",
    "\n",
    "    TOTAL_VULNS += len(vuln_commits)\n",
    "\n",
    "    commits_to_analyze: list[str] = []\n",
    "    commits_to_analyze.append(patch_commit)\n",
    "    commits_to_analyze.extend(vuln_commits)\n",
    "\n",
    "\n",
    "    temp_repo: Repository = Repository(remote_url, only_commits=commits_to_analyze, order='reverse')\n",
    "\n",
    "    patch_author_date: datetime = None\n",
    "    for commit in temp_repo.traverse_commits():\n",
    "        size_tracked: bool = False\n",
    "        commit_count: int = 0\n",
    "        \n",
    "        vuln_author_date: datetime = None\n",
    "\n",
    "        if commit_count == 0:\n",
    "            patch_author_date = commit.author_date\n",
    "            commit_count +=1\n",
    "        else: ### reassing the value of vuln_author_date on each iteration after count > 1\n",
    "            vuln_author_date = commit.author_date\n",
    "            commit_count += 1\n",
    "            TOTAL_PATCH_VULN_PAIRS += 1\n",
    "        ### Calculate difference between patch date and vuln date in months\n",
    "        difference: datetime = relativedelta(patch_author_date,vuln_author_date)\n",
    "\n",
    "        # Get the difference in months\n",
    "        months_difference = difference.years * 12 + difference.months\n",
    "\n",
    "        TOTAL_NUM_MONTHS += months_difference\n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ### Logic for point 1\n",
    "        if size_tracked == False:\n",
    "            temp_repo_path = commit.project_path  # Path to the cloned repo\n",
    "            repo_size: float = get_directory_size(temp_repo_path, SIZE_OF_CLONED_REPOS) / (1024 * 1024)  # Convert to MB\n",
    "            SIZE_OF_ALL_CLONED_REPOS += repo_size\n",
    "\n",
    "\n",
    "\n",
    "        ### Logic for point 3\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        shutil.rmtree(temp_repo_path)\n",
    "    \n",
    "    for vuln_commit in temp_repo_vulns.traverse_commits():\n",
    "        temp_repo_path = vuln_commit.project_path  # Path to the cloned repo\n",
    "        \n",
    "\n",
    "\n",
    "        ### Logic for point 3\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        shutil.rmtree(temp_repo_path)\n",
    "\n",
    "    count+=1\n",
    "\n",
    "AVERAGE_NUM_MONTHS_BETWEEN_VULN_N_PATCH = (TOTAL_NUM_MONTHS / TOTAL_PATCH_VULN_PAIRS )\n",
    "PERCENTAGE_OF_VULN_N_PATCH_BY_SAME_PERSON = (TOTAL_VULNS / BY_SAME_PERSON )\n",
    "\n",
    "### Point 2,Point 5\n",
    "TOTAL_PATCH_COMMITS_w_VULN_COMMIT: int = 0\n",
    "TOTAL_VULN_COMMITS_FOUND: int = 0\n",
    "TOTAL_NOT_FOUND: int = 0\n",
    "    \n",
    "\n",
    "AVERAGE_NUM_OF_VULNS_TO_PATCH: float = (TOTAL_VULN_COMMITS_FOUND / TAL_PATCH_COMMITS_w_VULN_COMMIT)\n",
    "\n",
    "\n",
    "### Checks\n",
    "assert(TOTAL_VULN_COMMITS_FOUND == TOTAL_VULNS)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydriller_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
