{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The following code analyzes the viable_patches_json file. The points of analysis are described below. The primary tool for this analysis is pydriller.\n",
    "\n",
    "1. Total size of the cloned repos\n",
    "2. Total number of vulnerability inducing commits (vuln commits) found & not found\n",
    "3. Average number of months between vuln commit and patch commit (or fix)\n",
    "4. Average number of commits between the vuln commit & patch commit (or fix)\n",
    "5. Average number of vuln commits fixed by patch commit (or fix)\n",
    "6. Percentage of vulns where the vuln commit and fix were made by the same person\n",
    "\n",
    "\n",
    "##### Sources\n",
    "- @inbook{PyDriller,\n",
    "    title = \"PyDriller: Python Framework for Mining Software Repositories\",\n",
    "    abstract = \"Software repositories contain historical and valuable information about the overall development of software systems. Mining software repositories (MSR) is nowadays considered one of the most interesting growing fields within software engineering. MSR focuses on extracting and analyzing data available in software repositories to uncover interesting, useful, and actionable information about the system. Even though MSR plays an important role in software engineering research, few tools have been created and made public to support developers in extracting information from Git repository. In this paper, we present PyDriller, a Python Framework that eases the process of mining Git. We compare our tool against the state-of-the-art Python Framework GitPython, demonstrating that PyDriller can achieve the same results with, on average, 50% less LOC and significantly lower complexity.URL: https://github.com/ishepard/pydrillerMaterials: https://doi.org/10.5281/zenodo.1327363Pre-print: https://doi.org/10.5281/zenodo.1327411\",\n",
    "    author = \"Spadini, Davide and Aniche, Maur√≠cio and Bacchelli, Alberto\",\n",
    "    year = \"2018\",\n",
    "    doi = \"10.1145/3236024.3264598\",\n",
    "    booktitle = \"The 26th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE)\",\n",
    "    }\n",
    "\n",
    "##### Author @Trust-Worthy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading in the results from the patch_vuln_match.json file and processing objects according to JSONL standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jsonlines    \n",
    "\n",
    "json_path:str = \"../../production_ready/patch_vuln_match.jsonl\"\n",
    "\n",
    "data: list[object] = []\n",
    "\n",
    "with jsonlines.open(json_path) as reader:\n",
    "\n",
    "    data = [entry for entry in reader]\n",
    "\n",
    "# Convert the list of dictionaries into a pandas DataFrame\n",
    "patch_vuln_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Define a function to extract the file paths and commits\n",
    "def extract_vuln_files_commits(vuln_commits):\n",
    "    if vuln_commits:\n",
    "        files = list(vuln_commits.keys())\n",
    "        commits = [commit for commits in vuln_commits.values() for commit in commits]\n",
    "        return pd.Series([files, commits])\n",
    "    else:\n",
    "        return pd.Series([[], []])  # Empty lists if no vuln_commits\n",
    "\n",
    "# # Apply the function to create new columns\n",
    "# patch_vuln_df[['vuln_files', 'vuln_commits']] = patch_vuln_df['vuln_commits'].apply(extract_vuln_files_commits)\n",
    "\n",
    "\n",
    "\n",
    "# print(patch_vuln_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is where the fun begins.... (iykyk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_paths(vuln_commits):\n",
    "    try:\n",
    "        if isinstance(vuln_commits, dict):\n",
    "            return list(vuln_commits.keys())\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting file paths: {e}\")\n",
    "        return []\n",
    "def extract_commit_hashes(vuln_commits):\n",
    "    try:\n",
    "        if isinstance(vuln_commits, dict):\n",
    "            return list({commit for commits in vuln_commits.values() if isinstance(commits, list) for commit in commits})\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting commit hashes: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           cve_id                     repo  \\\n",
      "0   CVE-1999-0199             bminor/glibc   \n",
      "1   CVE-1999-0731         KDE/kde1-kdebase   \n",
      "2   CVE-2002-2443                krb5/krb5   \n",
      "3  CVE-2005-10002  wp-plugins/secure-files   \n",
      "4  CVE-2005-10003      mikexstudios/xcomic   \n",
      "\n",
      "                               patch_commit  \\\n",
      "0  2864e767053317538feafa815046fff89e5a16be   \n",
      "1  04906bd5de2f220bf100b605dad37b4a1d9a91a6   \n",
      "2  cf1a0c411b2668c57c41e9c4efd15ba17b6b322c   \n",
      "3  cab025e5fc2bcdad8032d833ebc38e6bd2a13c92   \n",
      "4  6ed8e3cc336e29f09c7e791863d0559939da98bf   \n",
      "\n",
      "                                        vuln_commits  \\\n",
      "0  {'elf/dl-load.c': ['dc5efe83c0252ad45337ab98ef...   \n",
      "1                                                 {}   \n",
      "2  {'src/kadmin/server/schpw.c': ['e88f857c3680ea...   \n",
      "3  {'secure-files.php': ['b1afc063fd49cfb875e1c6f...   \n",
      "4                                                 {}   \n",
      "\n",
      "                                          vuln_files  \\\n",
      "0  [elf/dl-load.c, manual/search.texi, misc/syslo...   \n",
      "1                                                 []   \n",
      "2                        [src/kadmin/server/schpw.c]   \n",
      "3                                 [secure-files.php]   \n",
      "4                                                 []   \n",
      "\n",
      "                                         vuln_hashes  \n",
      "0  [569c558c880779d33c6642662d1aa57dff697244, 260...  \n",
      "1                                                 []  \n",
      "2  [43817ef775a04fb5694c39d9d6a27587ce2720a4, e88...  \n",
      "3         [b1afc063fd49cfb875e1c6f591543ebff6649469]  \n",
      "4                                                 []  \n",
      "              cve_id            repo  \\\n",
      "13378  CVE-2024-9287  python/cpython   \n",
      "13379  CVE-2024-9287  python/cpython   \n",
      "13380  CVE-2024-9287  python/cpython   \n",
      "13381  CVE-2024-9287  python/cpython   \n",
      "13382  CVE-2024-9287  python/cpython   \n",
      "\n",
      "                                   patch_commit  \\\n",
      "13378  e52095a0c1005a87eed2276af7a1f2f66e2b6483   \n",
      "13379  633555735a023d3e4d92ba31da35b1205f9ecbd7   \n",
      "13380  8450b2482586857d689b6658f08de9c8179af7db   \n",
      "13381  9286ab3a107ea41bd3f3c3682ce2512692bdded8   \n",
      "13382  ae961ae94bf19c8f8c7fbea3d1c25cc55ce8ae97   \n",
      "\n",
      "                                            vuln_commits  \\\n",
      "13378  {'Lib/venv/__init__.py': ['7ded1f0f694f0f99252...   \n",
      "13379  {'Lib/venv/__init__.py': ['7ded1f0f694f0f99252...   \n",
      "13380  {'Lib/venv/__init__.py': ['7ded1f0f694f0f99252...   \n",
      "13381  {'Lib/venv/__init__.py': ['7ded1f0f694f0f99252...   \n",
      "13382  {'Lib/test/test_venv.py': ['5e0df74b3bc6391e9a...   \n",
      "\n",
      "                                              vuln_files  \\\n",
      "13378  [Lib/venv/__init__.py, Lib/venv/scripts/common...   \n",
      "13379  [Lib/venv/__init__.py, Lib/venv/scripts/common...   \n",
      "13380  [Lib/venv/__init__.py, Lib/venv/scripts/common...   \n",
      "13381  [Lib/venv/__init__.py, Lib/venv/scripts/common...   \n",
      "13382  [Lib/test/test_venv.py, Lib/venv/__init__.py, ...   \n",
      "\n",
      "                                             vuln_hashes  \n",
      "13378  [574b324bdc9a126b5a4488c3613f11ad2555415e, dff...  \n",
      "13379  [574b324bdc9a126b5a4488c3613f11ad2555415e, dff...  \n",
      "13380  [574b324bdc9a126b5a4488c3613f11ad2555415e, dff...  \n",
      "13381  [574b324bdc9a126b5a4488c3613f11ad2555415e, dff...  \n",
      "13382  [574b324bdc9a126b5a4488c3613f11ad2555415e, dff...  \n"
     ]
    }
   ],
   "source": [
    "# Apply functions to create new columns\n",
    "patch_vuln_df[\"vuln_files\"] = patch_vuln_df[\"vuln_commits\"].apply(extract_file_paths)\n",
    "patch_vuln_df[\"vuln_hashes\"] = patch_vuln_df[\"vuln_commits\"].apply(extract_commit_hashes)\n",
    "\n",
    "\n",
    "\n",
    "print(patch_vuln_df.head())\n",
    "print(patch_vuln_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original vuln_commits column if not needed\n",
    "patch_vuln_df.drop(columns=[\"vuln_commits\"], inplace=True)\n",
    "\n",
    "print(patch_vuln_df.head())\n",
    "print(patch_vuln_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Global variables\n",
    "\"\"\"\n",
    "NVD_ALL_REPOS = \"/shared/rc/sfs/nvd-all-repos\"\n",
    "\n",
    "MATCH_FILES:str = \"../../production_ready/patch_vuln_match.jsonl\"\n",
    "\n",
    "### Point 1\n",
    "SIZE_OF_ALL_CLONED_REPOS: float = 0 ### size in MB\n",
    "\n",
    "### Point 2\n",
    "TOTAL_VULNS_COMMITS: int = 0 ### Another way to say this is total patch vuln pairs\n",
    "TOTAL_PATCH_COMMITS_W_VULN_COMMIT: int = 0\n",
    "\n",
    "\n",
    "### Point 6\n",
    "### I can get the the number of patches without vulns / not found by doing total entires - total vulns\n",
    "BY_SAME_PERSON: int = 0 ### Num of vulns made by the same person\n",
    "PERCENTAGE_OF_VULN_N_PATCH_BY_SAME_PERSON: float = 0.0\n",
    "\n",
    "\n",
    "### Point 3\n",
    "TOTAL_NUM_MONTHS_BETWEEN: int = 0\n",
    "AVERAGE_NUM_MONTHS_BETWEEN_VULN_N_PATCH: float = 0.0\n",
    "\n",
    "### Point 4\n",
    "TOTAL_NUM_COMMITS_BETWEEN: int = 0\n",
    "AVERAGE_NUM_COMMITS_BETWEEN_VULN_N_PATCH: float = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import glob\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "filename=\"test_jupyter_1.log\",\n",
    "level=logging.WARNING,\n",
    "format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "# Calculate repo size\n",
    "def get_directory_size(path: str) -> float:\n",
    "    size: float = 0\n",
    "    for dirpath, _, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            size += os.path.getsize(fp)\n",
    "    logging.info(f\"got the size for {path} repo\")\n",
    "    return size\n",
    "\n",
    "def safe_extract_vuln_files_commits(vuln_commits):\n",
    "    \"\"\"Wrapper function for error handling and logging.\"\"\"\n",
    "    try:\n",
    "        return extract_vuln_files_commits(vuln_commits)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing vuln_commits: {vuln_commits} - {e}\", exc_info=True)\n",
    "        return pd.Series([[], []])  # Return empty lists in case of failure\n",
    "def find_repo_path(owner_repo: str) -> str | None:\n",
    "    \"\"\"Finds the path of a repository inside NVD_ALL_REPOS.\n",
    "\n",
    "    Args:\n",
    "        owner_repo (str): The repository in 'owner/repo' format.\n",
    "\n",
    "    Returns:\n",
    "        str | None: The path to the repository if found, otherwise None.\n",
    "    \"\"\"\n",
    "    \n",
    "    matching_repos:list = glob.glob(os.path.join(NVD_ALL_REPOS, f\"*{owner_repo}*\"))\n",
    "    return matching_repos[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydriller import Repository\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta \n",
    "\n",
    "\n",
    "def calculate_all_repo_sizes(patch_vuln_df: pd.DataFrame,total_size:float) -> float:\n",
    "    \"\"\"\n",
    "\n",
    "    POINT 1\n",
    "    \n",
    "    This function calculates the total size of all the repos cloned for the Vulnerability History Project\n",
    "    in our NVD all repos database.\n",
    "\n",
    "    Args:\n",
    "        patch_vuln_df (pd.DataFrame): _description_\n",
    "        total_size (float): _description_\n",
    "\n",
    "    Returns:\n",
    "        float: _description_\n",
    "    \"\"\"\n",
    "    # Variable used to track repos analyzed for accurate storage metrics\n",
    "    unique_repo_paths: set[str] = set()\n",
    "\n",
    "    for repo in patch_vuln_df[\"repo\"]:\n",
    "        try:\n",
    "            repo_path = find_repo_path(repo)\n",
    "            if repo_path is None:\n",
    "                logging.error(f\"Repository path not found for {repo}, skipping this entry.\")\n",
    "                continue\n",
    "            logging.info(f\"Repository path for {repo}: {repo_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error finding repository path for {repo}: {e}\")\n",
    "            continue  # Skip this repo and move to the next\n",
    "        try:\n",
    "            # Code for point 1: Tracking repo size\n",
    "            if repo_path not in unique_repo_paths:\n",
    "                temp_repo_path = repo_path  \n",
    "                unique_repo_paths.add(temp_repo_path)\n",
    "                repo_size: float = get_directory_size(temp_repo_path) / (1024 * 1024)  # Convert to MB\n",
    "                total_size += repo_size\n",
    "                logging.info(f\"Repo size for {temp_repo_path} added. Total size: {total_size} MB\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error calculating repo size for {repo_path}: {e}\")\n",
    "            continue  # Continue to next commit if size calculation fails\n",
    "\n",
    "    return total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries in patch vuln dataframe:13383\n",
      "Total vulnerable commits in patch vuln dataframe:47979\n",
      "Total patch commits with at least one vulnerable commit:3112\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "POINT 2\n",
    "'''\n",
    "def calculate_total_num_vuln_hashes(patch_vuln_df: pd.DataFrame) -> int:\n",
    "    return patch_vuln_df[\"vuln_hashes\"].explode().count()\n",
    "\n",
    "def calculate_patch_vuln_matches(patch_vuln_df: pd.DataFrame) -> int:\n",
    "    \n",
    "    # Query for empty vuln_files and vuln_hashes\n",
    "    empty_patch_vuln_matches_count = patch_vuln_df[\n",
    "        #patch_vuln_df[\"vuln_files\"].apply(lambda x: len(x) == 0) #& \n",
    "        patch_vuln_df[\"vuln_hashes\"].apply(lambda x: len(x) == 0)\n",
    "    ].shape[0]\n",
    "    return empty_patch_vuln_matches_count\n",
    "\n",
    "TOTAL_VULNS_COMMITS = calculate_total_num_vuln_hashes(patch_vuln_df)\n",
    "TOTAL_PATCH_COMMITS_W_VULN_COMMIT = calculate_patch_vuln_matches(patch_vuln_df)\n",
    "TOTAL_DF_ENTRIES = patch_vuln_df.shape[0]\n",
    "print(\"Total entries in patch vuln dataframe:\" + str(TOTAL_DF_ENTRIES))\n",
    "print(\"Total vulnerable commits in patch vuln dataframe:\" + str(TOTAL_VULNS_COMMITS)) ### really total vuln patches\n",
    "print(\"Total patch commits with at least one vulnerable commit:\" + str(TOTAL_PATCH_COMMITS_W_VULN_COMMIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Point 3\n",
    "'''\n",
    "def calculate_total_num_months_between_patches_and_vulns(patch_vuln_df: pd.DataFrame) -> int:\n",
    "    for repo,patch_commit,vuln_hashes in zip(patch_vuln_df[\"repo\"],patch_vuln_df[\"patch_commit\"],patch_vuln_df[\"vuln_hashes\"]):\n",
    "        if vuln_hashes == []:\n",
    "            continue\n",
    "        \n",
    "        repo_path = find_repo_path(repo)\n",
    "        commits_to_analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 46)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<string>:46\u001b[0;36m\u001b[0m\n\u001b[0;31m    except Exception as e:\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "                # Handling patch commit (first commit in the list)\n",
    "                if is_patch:\n",
    "                    patch_author_date = commit.author_date\n",
    "                    patch_author = commit.author.email  # Email is typically a string\n",
    "                    patch_hash = commit.hash\n",
    "\n",
    "                    TOTAL_PATCH_COMMITS_W_VULN_COMMIT += 1\n",
    "                    logging.info(f\"Patch commit found: {patch_hash}. Total patches with vuln commits: {TOTAL_PATCH_COMMITS_W_VULN_COMMIT}\")\n",
    "                    is_patch = False\n",
    "                    continue  # Skip the patch commit in the next steps\n",
    "\n",
    "                # Handling vulnerability commit (following commits after patch)\n",
    "                vuln_author: Optional[str] = None\n",
    "                vuln_author_date: Optional[datetime] = commit.author_date\n",
    "                vuln_hash = commit.hash\n",
    "                vuln_author = commit.author.email\n",
    "\n",
    "                # Point 3: Calculate difference between patch and vuln dates in months\n",
    "                if patch_author_date and vuln_author_date:\n",
    "                    difference = relativedelta(patch_author_date, vuln_author_date)\n",
    "                    months_difference = (difference.years or 0) * 12 + (difference.months or 0)\n",
    "                    TOTAL_NUM_MONTHS_BETWEEN += months_difference\n",
    "                    logging.info(f\"Month difference between patch and vuln: {months_difference} months.\")\n",
    "                else:\n",
    "                    logging.warning(\"Missing date values for patch or vuln commit. Skipping date difference calculation.\")\n",
    "                \n",
    "                # Point 4: Count commits between patch and vuln commit\n",
    "                try:\n",
    "                    commit_count: int = get_commits_between(temp_repo_path, vuln_hash, patch_hash)\n",
    "                    TOTAL_NUM_COMMITS_BETWEEN += commit_count\n",
    "                    logging.info(f\"Commits between vuln and patch: {commit_count}. Total commits between: {TOTAL_NUM_COMMITS_BETWEEN}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error counting commits between {vuln_hash} and {patch_hash}: {e}\")\n",
    "                    continue  # Skip if commit counting fails\n",
    "\n",
    "                # Point 6: Compare patch and vuln author\n",
    "                if patch_author == vuln_author:\n",
    "                    BY_SAME_PERSON += 1\n",
    "                    logging.info(f\"Patch and vuln by same author: {patch_author}. Total: {BY_SAME_PERSON}\")\n",
    "\n",
    "                # Point 2: Get total number of vulnerabilities\n",
    "                TOTAL_VULNS += len(vuln_commits)\n",
    "                logging.info(f\"Total vulnerabilities so far: {TOTAL_VULNS}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing commit {commit.hash} in repo {owner_repo}: {e}\")\n",
    "                continue  # Skip this commit and continue to next one\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydriller_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
