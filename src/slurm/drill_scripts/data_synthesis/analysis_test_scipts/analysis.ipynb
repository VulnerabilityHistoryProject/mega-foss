{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The following code analyzes the viable_patches_json file. The points of analysis are described below. The primary tool for this analysis is pydriller.\n",
    "\n",
    "1. Total size of the cloned repos\n",
    "2. Total number of vulnerability inducing commits (vuln commits) found & not found\n",
    "3. Average number of months between vuln commit and patch commit (or fix)\n",
    "4. Average number of commits between the vuln commit & patch commit (or fix)\n",
    "5. Average number of vuln commits fixed by patch commit (or fix)\n",
    "6. Percentage of vulns where the vuln commit and fix were made by the same person\n",
    "\n",
    "\n",
    "##### Sources\n",
    "- @inbook{PyDriller,\n",
    "    title = \"PyDriller: Python Framework for Mining Software Repositories\",\n",
    "    abstract = \"Software repositories contain historical and valuable information about the overall development of software systems. Mining software repositories (MSR) is nowadays considered one of the most interesting growing fields within software engineering. MSR focuses on extracting and analyzing data available in software repositories to uncover interesting, useful, and actionable information about the system. Even though MSR plays an important role in software engineering research, few tools have been created and made public to support developers in extracting information from Git repository. In this paper, we present PyDriller, a Python Framework that eases the process of mining Git. We compare our tool against the state-of-the-art Python Framework GitPython, demonstrating that PyDriller can achieve the same results with, on average, 50% less LOC and significantly lower complexity.URL: https://github.com/ishepard/pydrillerMaterials: https://doi.org/10.5281/zenodo.1327363Pre-print: https://doi.org/10.5281/zenodo.1327411\",\n",
    "    author = \"Spadini, Davide and Aniche, Maur√≠cio and Bacchelli, Alberto\",\n",
    "    year = \"2018\",\n",
    "    doi = \"10.1145/3236024.3264598\",\n",
    "    booktitle = \"The 26th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE)\",\n",
    "    }\n",
    "\n",
    "##### Author @Trust-Worthy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading in the results from the patch_vuln_match.json file and processing objects according to JSONL standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jsonlines    \n",
    "\n",
    "json_path:str = \"../../production_ready/patch_vuln_match.jsonl\"\n",
    "\n",
    "data: list[object] = []\n",
    "\n",
    "with jsonlines.open(json_path) as reader:\n",
    "\n",
    "    data = [entry for entry in reader]\n",
    "\n",
    "# Convert the list of dictionaries into a pandas DataFrame\n",
    "patch_vuln_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Define a function to extract the file paths and commits\n",
    "def extract_vuln_files_commits(vuln_commits):\n",
    "    if vuln_commits:\n",
    "        files = list(vuln_commits.keys())\n",
    "        commits = [commit for commits in vuln_commits.values() for commit in commits]\n",
    "        return pd.Series([files, commits])\n",
    "    else:\n",
    "        return pd.Series([[], []])  # Empty lists if no vuln_commits\n",
    "\n",
    "# # Apply the function to create new columns\n",
    "# patch_vuln_df[['vuln_files', 'vuln_commits']] = patch_vuln_df['vuln_commits'].apply(extract_vuln_files_commits)\n",
    "\n",
    "\n",
    "\n",
    "# print(patch_vuln_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is where the fun begins.... (iykyk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_paths(vuln_commits):\n",
    "    try:\n",
    "        if isinstance(vuln_commits, dict):\n",
    "            return list(vuln_commits.keys())\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting file paths: {e}\")\n",
    "        return []\n",
    "def extract_commit_hashes(vuln_commits):\n",
    "    try:\n",
    "        if isinstance(vuln_commits, dict):\n",
    "            return list({commit for commits in vuln_commits.values() if isinstance(commits, list) for commit in commits})\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting commit hashes: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           cve_id                     repo  \\\n",
      "0   CVE-1999-0199             bminor/glibc   \n",
      "1   CVE-1999-0731         KDE/kde1-kdebase   \n",
      "2   CVE-2002-2443                krb5/krb5   \n",
      "3  CVE-2005-10002  wp-plugins/secure-files   \n",
      "4  CVE-2005-10003      mikexstudios/xcomic   \n",
      "\n",
      "                               patch_commit  \\\n",
      "0  2864e767053317538feafa815046fff89e5a16be   \n",
      "1  04906bd5de2f220bf100b605dad37b4a1d9a91a6   \n",
      "2  cf1a0c411b2668c57c41e9c4efd15ba17b6b322c   \n",
      "3  cab025e5fc2bcdad8032d833ebc38e6bd2a13c92   \n",
      "4  6ed8e3cc336e29f09c7e791863d0559939da98bf   \n",
      "\n",
      "                                        vuln_commits  \\\n",
      "0  {'elf/dl-load.c': ['dc5efe83c0252ad45337ab98ef...   \n",
      "1                                                 {}   \n",
      "2  {'src/kadmin/server/schpw.c': ['e88f857c3680ea...   \n",
      "3  {'secure-files.php': ['b1afc063fd49cfb875e1c6f...   \n",
      "4                                                 {}   \n",
      "\n",
      "                                          vuln_files  \\\n",
      "0  [elf/dl-load.c, manual/search.texi, misc/syslo...   \n",
      "1                                                 []   \n",
      "2                        [src/kadmin/server/schpw.c]   \n",
      "3                                 [secure-files.php]   \n",
      "4                                                 []   \n",
      "\n",
      "                                         vuln_hashes  \n",
      "0  [569c558c880779d33c6642662d1aa57dff697244, 260...  \n",
      "1                                                 []  \n",
      "2  [43817ef775a04fb5694c39d9d6a27587ce2720a4, e88...  \n",
      "3         [b1afc063fd49cfb875e1c6f591543ebff6649469]  \n",
      "4                                                 []  \n",
      "              cve_id            repo  \\\n",
      "13378  CVE-2024-9287  python/cpython   \n",
      "13379  CVE-2024-9287  python/cpython   \n",
      "13380  CVE-2024-9287  python/cpython   \n",
      "13381  CVE-2024-9287  python/cpython   \n",
      "13382  CVE-2024-9287  python/cpython   \n",
      "\n",
      "                                   patch_commit  \\\n",
      "13378  e52095a0c1005a87eed2276af7a1f2f66e2b6483   \n",
      "13379  633555735a023d3e4d92ba31da35b1205f9ecbd7   \n",
      "13380  8450b2482586857d689b6658f08de9c8179af7db   \n",
      "13381  9286ab3a107ea41bd3f3c3682ce2512692bdded8   \n",
      "13382  ae961ae94bf19c8f8c7fbea3d1c25cc55ce8ae97   \n",
      "\n",
      "                                            vuln_commits  \\\n",
      "13378  {'Lib/venv/__init__.py': ['7ded1f0f694f0f99252...   \n",
      "13379  {'Lib/venv/__init__.py': ['7ded1f0f694f0f99252...   \n",
      "13380  {'Lib/venv/__init__.py': ['7ded1f0f694f0f99252...   \n",
      "13381  {'Lib/venv/__init__.py': ['7ded1f0f694f0f99252...   \n",
      "13382  {'Lib/test/test_venv.py': ['5e0df74b3bc6391e9a...   \n",
      "\n",
      "                                              vuln_files  \\\n",
      "13378  [Lib/venv/__init__.py, Lib/venv/scripts/common...   \n",
      "13379  [Lib/venv/__init__.py, Lib/venv/scripts/common...   \n",
      "13380  [Lib/venv/__init__.py, Lib/venv/scripts/common...   \n",
      "13381  [Lib/venv/__init__.py, Lib/venv/scripts/common...   \n",
      "13382  [Lib/test/test_venv.py, Lib/venv/__init__.py, ...   \n",
      "\n",
      "                                             vuln_hashes  \n",
      "13378  [574b324bdc9a126b5a4488c3613f11ad2555415e, dff...  \n",
      "13379  [574b324bdc9a126b5a4488c3613f11ad2555415e, dff...  \n",
      "13380  [574b324bdc9a126b5a4488c3613f11ad2555415e, dff...  \n",
      "13381  [574b324bdc9a126b5a4488c3613f11ad2555415e, dff...  \n",
      "13382  [574b324bdc9a126b5a4488c3613f11ad2555415e, dff...  \n"
     ]
    }
   ],
   "source": [
    "# Apply functions to create new columns\n",
    "patch_vuln_df[\"vuln_files\"] = patch_vuln_df[\"vuln_commits\"].apply(extract_file_paths)\n",
    "patch_vuln_df[\"vuln_hashes\"] = patch_vuln_df[\"vuln_commits\"].apply(extract_commit_hashes)\n",
    "\n",
    "\n",
    "\n",
    "print(patch_vuln_df.head())\n",
    "print(patch_vuln_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original vuln_commits column if not needed\n",
    "patch_vuln_df.drop(columns=[\"vuln_commits\"], inplace=True)\n",
    "\n",
    "print(patch_vuln_df.head())\n",
    "print(patch_vuln_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Global variables\n",
    "\"\"\"\n",
    "NVD_ALL_REPOS = \"/shared/rc/sfs/nvd-all-repos\"\n",
    "\n",
    "MATCH_FILES:str = \"../../production_ready/patch_vuln_match.jsonl\"\n",
    "\n",
    "### Point 1\n",
    "SIZE_OF_ALL_CLONED_REPOS: float = 0 ### size in MB\n",
    "\n",
    "### Point 2\n",
    "TOTAL_VULNS_COMMITS: int = 0 ### Another way to say this is total patch vuln pairs\n",
    "TOTAL_PATCH_COMMITS_W_VULN_COMMIT: int = 0\n",
    "\n",
    "\n",
    "### Point 6\n",
    "### I can get the the number of patches without vulns / not found by doing total entires - total vulns\n",
    "BY_SAME_PERSON: int = 0 ### Num of vulns made by the same person\n",
    "PERCENTAGE_OF_VULN_N_PATCH_BY_SAME_PERSON: float = 0.0\n",
    "\n",
    "\n",
    "### Point 3\n",
    "TOTAL_NUM_MONTHS_BETWEEN: int = 0\n",
    "AVERAGE_NUM_MONTHS_BETWEEN_VULN_N_PATCH: float = 0.0\n",
    "\n",
    "### Point 4\n",
    "TOTAL_NUM_COMMITS_BETWEEN: int = 0\n",
    "AVERAGE_NUM_COMMITS_BETWEEN_VULN_N_PATCH: float = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import glob\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "filename=\"test_jupyter_1.log\",\n",
    "level=logging.WARNING,\n",
    "format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "# Calculate repo size\n",
    "def get_directory_size(path: str) -> float:\n",
    "    size: float = 0\n",
    "    for dirpath, _, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            size += os.path.getsize(fp)\n",
    "    logging.info(f\"got the size for {path} repo\")\n",
    "    return size\n",
    "\n",
    "def safe_extract_vuln_files_commits(vuln_commits):\n",
    "    \"\"\"Wrapper function for error handling and logging.\"\"\"\n",
    "    try:\n",
    "        return extract_vuln_files_commits(vuln_commits)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing vuln_commits: {vuln_commits} - {e}\", exc_info=True)\n",
    "        return pd.Series([[], []])  # Return empty lists in case of failure\n",
    "def find_repo_path(owner_repo: str) -> str | None:\n",
    "    \"\"\"Finds the path of a repository inside NVD_ALL_REPOS.\n",
    "\n",
    "    Args:\n",
    "        owner_repo (str): The repository in 'owner/repo' format.\n",
    "\n",
    "    Returns:\n",
    "        str | None: The path to the repository if found, otherwise None.\n",
    "    \"\"\"\n",
    "    \n",
    "    matching_repos:list = glob.glob(os.path.join(NVD_ALL_REPOS, f\"*{owner_repo}*\"))\n",
    "    return matching_repos[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydriller import Repository\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta \n",
    "'''\n",
    "POINT 1\n",
    "\n",
    "'''\n",
    "\n",
    "def calculate_all_repo_sizes(patch_vuln_df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the total disk size of all unique repositories in the given DataFrame.\n",
    "\n",
    "    This function iterates through the \"repo\" column, determines their local paths,\n",
    "    and accumulates their sizes (in MB), ensuring each repo is counted only once.\n",
    "\n",
    "    Args:\n",
    "        patch_vuln_df (pd.DataFrame): DataFrame containing repo names.\n",
    "\n",
    "    Returns:\n",
    "        float: Total repository size in MB.\n",
    "    \"\"\"\n",
    "    unique_repo_paths: set[str] = set()\n",
    "    total_size: float = 0.0  # Initialize total size inside the function\n",
    "\n",
    "    for repo in patch_vuln_df[\"repo\"]:\n",
    "        try:\n",
    "            repo_path = find_repo_path(repo)\n",
    "            if not repo_path:\n",
    "                logging.warning(f\"Repository path not found for {repo}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            if repo_path in unique_repo_paths:\n",
    "                continue  # Skip if already counted\n",
    "\n",
    "            unique_repo_paths.add(repo_path)\n",
    "            repo_size: float = get_directory_size(repo_path) / (1024 * 1024)  # Convert to MB\n",
    "            total_size += repo_size\n",
    "\n",
    "            logging.info(f\"Added {repo_path} ({repo_size:.2f} MB). Total: {total_size:.2f} MB\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {repo}: {e}\")\n",
    "            continue  # Move to the next repo\n",
    "\n",
    "    return total_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries in patch vuln dataframe:13383\n",
      "Total vulnerable commits in patch vuln dataframe:47979\n",
      "Total patch commits with at least one vulnerable commit:10271\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "POINT 2\n",
    "'''\n",
    "def calculate_total_num_vuln_hashes(patch_vuln_df: pd.DataFrame) -> int:\n",
    "    return patch_vuln_df[\"vuln_hashes\"].explode().count()\n",
    "\n",
    "def calculate_patch_vuln_matches(patch_vuln_df: pd.DataFrame) -> int:\n",
    "    \n",
    "    # Query for empty vuln_files and vuln_hashes\n",
    "    empty_patch_vuln_matches_count = patch_vuln_df[\n",
    "        #patch_vuln_df[\"vuln_files\"].apply(lambda x: len(x) == 0) #& \n",
    "        patch_vuln_df[\"vuln_hashes\"].apply(lambda x: len(x) > 0)\n",
    "    ].shape[0]\n",
    "    return empty_patch_vuln_matches_count\n",
    "\n",
    "TOTAL_VULNS_COMMITS = calculate_total_num_vuln_hashes(patch_vuln_df)\n",
    "TOTAL_PATCH_COMMITS_W_VULN_COMMIT = calculate_patch_vuln_matches(patch_vuln_df)\n",
    "TOTAL_DF_ENTRIES = patch_vuln_df.shape[0]\n",
    "print(\"Total entries in patch vuln dataframe:\" + str(TOTAL_DF_ENTRIES))\n",
    "print(\"Total vulnerable commits in patch vuln dataframe:\" + str(TOTAL_VULNS_COMMITS)) ### really total vuln patches\n",
    "print(\"Total patch commits with at least one vulnerable commit:\" + str(TOTAL_PATCH_COMMITS_W_VULN_COMMIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Point 3\n",
    "'''\n",
    "non_empty_vuln_hashes_df = patch_vuln_df[patch_vuln_df[\"vuln_hashes\"].apply(lambda x: len(x) > 0)].copy()\n",
    "\n",
    "\n",
    "def calculate_total_num_months_between_patch_and_vulns(non_empty_vuln_hashes: pd.DataFrame) -> int:\n",
    "    total = 0\n",
    "\n",
    "    for repo, patch_commit_hash, vuln_hashes in zip(non_empty_vuln_hashes[\"repo\"], non_empty_vuln_hashes[\"patch_commit\"], non_empty_vuln_hashes[\"vuln_hashes\"]):\n",
    "        \n",
    "        repo_path = find_repo_path(repo)\n",
    "        # repo_url = \"https://github.com/\" + repo\n",
    "        commits_to_analyze = [patch_commit_hash] + vuln_hashes  # Add patch commit + all vuln commits\n",
    "\n",
    "        if not patch_commit_hash or not vuln_hashes:\n",
    "            logging.warning(f\"Skipping {repo} due to missing patch or vulnerability hashes.\")\n",
    "            continue\n",
    "        \n",
    "        try:    \n",
    "            REPOSITORY = Repository(repo_path, only_commits=commits_to_analyze, order='reverse')\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize repository for {repo}: {e}\")\n",
    "            continue\n",
    "\n",
    "        patch_commit_date = None\n",
    "        total_diff_in_months = 0\n",
    "\n",
    "        # Process commits one by one\n",
    "        for commit in REPOSITORY.traverse_commits():\n",
    "            if commit.hash == patch_commit_hash:\n",
    "                patch_commit_date = commit.author_date  # Store patch commit date\n",
    "                continue\n",
    "\n",
    "            if patch_commit_date:\n",
    "                total_diff_in_months += abs((patch_commit_date - commit.author_date).days) / 30.44  # Convert days to months\n",
    "        \n",
    "        total += total_diff_in_months\n",
    "\n",
    "    return total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "POINT 4\n",
    "'''\n",
    "\n",
    "\n",
    "def calculate_total_num_commits_between_patch_and_vulns(non_empty_vuln_hashes: pd.DataFrame) -> int:\n",
    "    \n",
    "    total_commits_between: int = 0\n",
    "    for repo,patch_commit_hash,vuln_hashes in zip(non_empty_vuln_hashes[\"repo\"],non_empty_vuln_hashes[\"patch_commit\"],non_empty_vuln_hashes[\"vuln_hashes\"]):\n",
    "        \n",
    "        repo_path: str = find_repo_path(repo)\n",
    "        \n",
    "        for vuln_hash in vuln_hashes:\n",
    "            try:\n",
    "                REPOSITORY: Repository = Repository(repo_path, from_commit=vuln_hash, to_commit=patch_commit_hash, order='reverse')\n",
    "                num_commits_between = sum(1 for _ in REPOSITORY.traverse_commits())\n",
    "                total_commits_between += num_commits_between\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to initialize repository for {repo}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return total_commits_between\n",
    "            \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "POINT 5\n",
    "'''\n",
    "\n",
    "def calculate_average_num_vuln_commits_fixed_by_patch_commit(\n",
    "        total_vulns,patch_vuln_matches) -> float:\n",
    "    return total_vulns / patch_vuln_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "POINT 6\n",
    "'''\n",
    "\n",
    "def calculate_num_vulns_made_and_fixed_by_same_person(non_empty_vuln_hashes: pd.DataFrame)->int:\n",
    "    total = 0\n",
    "\n",
    "    for repo, patch_commit_hash, vuln_hashes in zip(non_empty_vuln_hashes[\"repo\"], non_empty_vuln_hashes[\"patch_commit\"], non_empty_vuln_hashes[\"vuln_hashes\"]):\n",
    "        \n",
    "        repo_path = find_repo_path(repo)\n",
    "        commits_to_analyze = [patch_commit_hash] + vuln_hashes  # Add patch commit + all vuln commits\n",
    "\n",
    "        if not patch_commit_hash or not vuln_hashes:\n",
    "            logging.warning(f\"Skipping {repo} due to missing patch or vulnerability hashes.\")\n",
    "            continue\n",
    "        \n",
    "        try:    \n",
    "            REPOSITORY = Repository(str(repo_path), only_commits=commits_to_analyze, order='reverse')\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize repository for {repo}: {e}\")\n",
    "            continue\n",
    "\n",
    "        commit = next(Repository(repo_path, single=patch_commit_hash).traverse_commits())\n",
    "        patch_commit_author = commit.author.email\n",
    "\n",
    "        occurances: int = 0\n",
    "\n",
    "        # Process commits one by one\n",
    "        for commit in REPOSITORY.traverse_commits():\n",
    "            if commit.author.email == patch_commit_author:\n",
    "                occurances += 1\n",
    "\n",
    "\n",
    "        total += occurances\n",
    "\n",
    "    return total\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydriller_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
