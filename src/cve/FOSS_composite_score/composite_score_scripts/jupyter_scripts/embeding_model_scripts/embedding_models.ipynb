{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87603860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "# Load BGE model\n",
    "model_name = \"BAAI/bge-large-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Sample prompt\n",
    "prompt = \"Flask offers suggestions, but doesn't enforce any dependencies or project layout. \" \\\n",
    "    \"It is up to the developer to choose the tools and libraries they want to use. \" \\\n",
    "    \"There are many extensions provided by the community that make adding new functionality easy.\"\n",
    "    \n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# === Embed inputs ===\n",
    "embedding_layer = model.get_input_embeddings()\n",
    "input_embeddings = embedding_layer(input_ids)  # shape: [1, seq_len, hidden_dim]\n",
    "input_embeddings.requires_grad_()\n",
    "\n",
    "# === Custom forward: dot product with concept vector ===\n",
    "concept_vector = torch.ones(model.config.hidden_size)  # or define a better one\n",
    "\n",
    "def custom_forward(embeds):\n",
    "    outputs = model(inputs_embeds=embeds, attention_mask=attention_mask)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_dim]\n",
    "    return torch.matmul(cls_embedding, concept_vector.to(cls_embedding.device))  # [batch_size]\n",
    "\n",
    "# === Captum Integrated Gradients ===\n",
    "ig = IntegratedGradients(custom_forward)\n",
    "attributions, delta = ig.attribute(input_embeddings, return_convergence_delta=True)\n",
    "\n",
    "# === Tokens and attribution scores ===\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze())\n",
    "token_attributions = attributions.squeeze().sum(dim=-1)  # sum across embedding dim\n",
    "\n",
    "# === Print ===\n",
    "for token, score in zip(tokens, token_attributions):\n",
    "    print(f\"{token:>12} : {score.item():.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd710c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Print ===\n",
    "for token, score in zip(tokens, token_attributions):\n",
    "    print(f\"{token:>12} : {score.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydriller_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
