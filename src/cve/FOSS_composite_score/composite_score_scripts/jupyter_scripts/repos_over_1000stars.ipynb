{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time \n",
    "import json\n",
    "load_dotenv()\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "HEADERS = {\"Authorization\": f\"Bearer {GITHUB_TOKEN}\"}\n",
    "API_URL = \"https://api.github.com/graphql\"\n",
    "\n",
    "query = \"\"\"\n",
    "query ($after: String) {\n",
    "  search(query: \"stars:>1000\", type: REPOSITORY, first: 50, after: $after) {\n",
    "    repositoryCount\n",
    "    pageInfo {\n",
    "      endCursor\n",
    "      hasNextPage\n",
    "    }\n",
    "    edges {\n",
    "      node {\n",
    "        ... on Repository {\n",
    "          nameWithOwner\n",
    "          description\n",
    "          stargazerCount\n",
    "          forkCount\n",
    "          url\n",
    "          updatedAt\n",
    "          issues(states: OPEN) {\n",
    "            totalCount\n",
    "          }\n",
    "          watchers {\n",
    "            totalCount\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_data = []\n",
    "cursor = None\n",
    "\n",
    "while True:\n",
    "    variables = {\"after\": cursor}\n",
    "    response = requests.post(API_URL, json={\"query\": query, \"variables\": variables}, headers=HEADERS)\n",
    "    data = response.json()\n",
    "    print(data)\n",
    "\n",
    "    # Optional: basic error handling\n",
    "    if \"errors\" in data:\n",
    "        print(\"GraphQL error:\", data[\"errors\"])\n",
    "        break\n",
    "\n",
    "    return_data.append(data)\n",
    "\n",
    "    page_info = data['data']['search']['pageInfo']\n",
    "    if not page_info[\"hasNextPage\"]:\n",
    "        break\n",
    "\n",
    "    cursor = page_info[\"endCursor\"]\n",
    "\n",
    "    # Optional: sleep to respect rate limits\n",
    "    time.sleep(1)\n",
    "\n",
    "# Write all responses to a file\n",
    "with open(\"github_response.json\", \"w\") as f:\n",
    "    json.dump(return_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURSOR_FILE = 'github_cursor.json'\n",
    "DATA_FILE = 'github_response_2.json'\n",
    "\n",
    "def save_state(cursor, data):\n",
    "    with open(CURSOR_FILE, '+a') as f:\n",
    "        json.dump({'cursor': cursor}, f)\n",
    "    with open(DATA_FILE, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "def load_state():\n",
    "    if os.path.exists(CURSOR_FILE) and os.path.exists(DATA_FILE):\n",
    "        with open(CURSOR_FILE) as f:\n",
    "            cursor = json.load(f).get('cursor')\n",
    "        with open(DATA_FILE) as f:\n",
    "            data = json.load(f)\n",
    "        return cursor, data\n",
    "    return None, []\n",
    "\n",
    "def run_query():\n",
    "    cursor, return_data = load_state()\n",
    "    retry_delay = 1\n",
    "    max_delay = 300\n",
    "\n",
    "    while True:\n",
    "        variables = {'after': cursor}\n",
    "        try:\n",
    "            response = requests.post(API_URL, json={'query': query, 'variables': variables}, headers=HEADERS)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f'Connection error: {e}, retrying in {retry_delay}s...')\n",
    "            time.sleep(retry_delay)\n",
    "            retry_delay = min(retry_delay * 2, max_delay)\n",
    "            continue\n",
    "\n",
    "        if response.status_code == 403:\n",
    "            reset = int(response.headers.get('X-RateLimit-Reset', time.time() + 60))\n",
    "            wait_time = max(reset - time.time(), 60)\n",
    "            print(f'Rate limited! Sleeping for {wait_time:.2f} seconds...')\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f'Error {response.status_code}: {response.text}, retrying in {retry_delay}s...')\n",
    "            time.sleep(retry_delay)\n",
    "            retry_delay = min(retry_delay * 2, max_delay)\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        if 'errors' in data:\n",
    "            print('GraphQL error:', data['errors'])\n",
    "            break\n",
    "\n",
    "        return_data.append(data)\n",
    "        save_state(cursor, return_data)\n",
    "\n",
    "        page_info = data['data']['search']['pageInfo']\n",
    "        if not page_info['hasNextPage']:\n",
    "            print('No more pages to fetch.')\n",
    "            break\n",
    "\n",
    "        cursor = page_info['endCursor']\n",
    "        retry_delay = 1  # reset delay after success\n",
    "        time.sleep(1)    # be nice to GitHub\n",
    "\n",
    "    print(f'Fetched {len(return_data)} pages of data.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_query()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "FOSS project name, description, stars, forks, issues, watchers\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open('github_response.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Prepare the list to hold the rows for the CSV\n",
    "rows = []\n",
    "\n",
    "# Loop through the search results and extract the relevant fields\n",
    "for item in data:\n",
    "    for repo in item['data']['search']['edges']:\n",
    "        repo_data = repo['node']\n",
    "        rows.append([\n",
    "            repo_data['nameWithOwner'],\n",
    "            repo_data['description'],\n",
    "            repo_data['stargazerCount'],\n",
    "            repo_data['forkCount'],\n",
    "            repo_data['issues']['totalCount'],\n",
    "            repo_data['watchers']['totalCount']\n",
    "        ])\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('github_repositories.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write the header\n",
    "    writer.writerow([\"FOSS project name\", \"description\", \"stars\", \"forks\", \"issues\", \"watchers\"])\n",
    "    # Write the rows\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(\"Data successfully written to github_repositories.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://api.github.com/graphql\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {os.environ['GITHUB_TOKEN']}\"\n",
    "}\n",
    "OUTPUT_DIR = \"github_data\"\n",
    "STEP = 1000\n",
    "\n",
    "query_template = \"\"\"\n",
    "query($after: String, $range: String!) {\n",
    "  search(query: $range, type: REPOSITORY, first: 50, after: $after) {\n",
    "    repositoryCount\n",
    "    pageInfo {\n",
    "      endCursor\n",
    "      hasNextPage\n",
    "    }\n",
    "    edges {\n",
    "      node {\n",
    "        ... on Repository {\n",
    "          nameWithOwner\n",
    "          description\n",
    "          stargazerCount\n",
    "          forkCount\n",
    "          url\n",
    "          updatedAt\n",
    "          issues {\n",
    "            totalCount\n",
    "          }\n",
    "          watchers {\n",
    "            totalCount\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def generate_star_ranges(start=1000, end=450000, step=1000):\n",
    "    ranges = []\n",
    "    for s in range(start, end, step):\n",
    "        e = s + step - 1\n",
    "        ranges.append(f\"stars:{s}..{e}\")\n",
    "    ranges.append(f\"stars:>={end}\")\n",
    "    return ranges\n",
    "\n",
    "def save_state(star_range, cursor, data):\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    prefix = star_range.replace(\":\", \"_\").replace(\"..\", \"_to_\").replace(\">=\", \"gte\")\n",
    "    with open(os.path.join(OUTPUT_DIR, f\"{prefix}_cursor.json\"), \"w\") as f:\n",
    "        json.dump({\"cursor\": cursor}, f)\n",
    "    with open(os.path.join(OUTPUT_DIR, f\"{prefix}_data.json\"), \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "def load_state(star_range):\n",
    "    prefix = star_range.replace(\":\", \"_\").replace(\"..\", \"_to_\").replace(\">=\", \"gte\")\n",
    "    cursor_path = os.path.join(OUTPUT_DIR, f\"{prefix}_cursor.json\")\n",
    "    data_path = os.path.join(OUTPUT_DIR, f\"{prefix}_data.json\")\n",
    "    if os.path.exists(cursor_path) and os.path.exists(data_path):\n",
    "        with open(cursor_path) as f:\n",
    "            cursor = json.load(f).get(\"cursor\")\n",
    "        with open(data_path) as f:\n",
    "            data = json.load(f)\n",
    "        return cursor, data\n",
    "    return None, []\n",
    "\n",
    "def fetch_star_range(star_range):\n",
    "    cursor, all_data = load_state(star_range)\n",
    "    retry_delay = 1\n",
    "    max_delay = 300\n",
    "    print(f\"\\n‚ñ∂ Fetching range: {star_range}\")\n",
    "\n",
    "    while True:\n",
    "        variables = {\n",
    "            \"after\": cursor,\n",
    "            \"range\": f\"{star_range} sort:stars-desc\"\n",
    "        }\n",
    "        try:\n",
    "            res = requests.post(API_URL, json={\"query\": query_template, \"variables\": variables}, headers=HEADERS)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Network error: {e}. Retrying in {retry_delay}s...\")\n",
    "            time.sleep(retry_delay)\n",
    "            retry_delay = min(retry_delay * 2, max_delay)\n",
    "            continue\n",
    "\n",
    "        if res.status_code == 403:\n",
    "            reset = int(res.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = max(reset - time.time(), 60)\n",
    "            print(f\"‚è≥ Rate limited. Sleeping for {wait_time:.1f}s...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            print(f\"‚ùå Error {res.status_code}: {res.text}\")\n",
    "            time.sleep(retry_delay)\n",
    "            retry_delay = min(retry_delay * 2, max_delay)\n",
    "            continue\n",
    "\n",
    "        data = res.json()\n",
    "        if \"errors\" in data:\n",
    "            print(f\"‚ùå GraphQL error: {data['errors']}\")\n",
    "            break\n",
    "\n",
    "        all_data.append(data)\n",
    "        page_info = data[\"data\"][\"search\"][\"pageInfo\"]\n",
    "        cursor = page_info[\"endCursor\"]\n",
    "        save_state(star_range, cursor, all_data)\n",
    "\n",
    "        if not page_info[\"hasNextPage\"]:\n",
    "            print(f\"‚úÖ Finished range: {star_range}. Pages fetched: {len(all_data)}\")\n",
    "            break\n",
    "\n",
    "        retry_delay = 1  # Reset retry delay on success\n",
    "        time.sleep(1)\n",
    "\n",
    "def run_all():\n",
    "    ranges = generate_star_ranges(1000, 450000, STEP)\n",
    "    for r in ranges:\n",
    "        fetch_star_range(r)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "\n",
    "github_stats = Path(\"../github_data\")\n",
    "\n",
    "# Prepare the list to hold the rows for the CSV\n",
    "rows = []\n",
    "\n",
    "for data_file in github_stats.iterdir():\n",
    "    if data_file.is_file():\n",
    "        \n",
    "        with open(data_file) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "\n",
    "        # Loop through the search results and extract the relevant fields\n",
    "        for item in data:\n",
    "            # if isinstance(item, str):\n",
    "            #     item = json.load(item)\n",
    "            try:\n",
    "                for repo in item['data']['search']['edges']:\n",
    "                    repo_data = repo['node']\n",
    "                    rows.append([\n",
    "                        repo_data['nameWithOwner'],\n",
    "                        repo_data['description'],\n",
    "                        repo_data['stargazerCount'],\n",
    "                        repo_data['forkCount'],\n",
    "                        repo_data['issues']['totalCount'],\n",
    "                        repo_data['watchers']['totalCount'],\n",
    "                        repo_data['updatedAt']\n",
    "                    ])\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {data_file.name}: {e}\")\n",
    "        \n",
    "with open(\"../csv_github_data/final_github_repos.csv\",'w',newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"FOSS project name\", \"description\", \"stars\", \"forks\", \"issues\", \"watchers\",\"last_ updated\"])\n",
    "    # Write the rows\n",
    "    writer.writerows(rows)\n",
    "    \n",
    "   \n",
    "print(\"Data successfully written to github_repositories_final.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import fnmatch\n",
    "\n",
    "# Config \n",
    "CSV_PATH = \"../csv_github_data/FOSS_projects_slash.csv\"\n",
    "OUTPUT_PATH = \"../enriched_github_data/FOSS_projects_with_dependencies.json\"\n",
    "GITHUB_TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
    "\n",
    "# Dependency filenames\n",
    "dependency_files = [\n",
    "    \"package.json\", \"package-lock.json\", \"yarn.lock\", \"pnpm-lock.yaml\",\n",
    "    \"requirements.txt\", \"setup.py\", \"pyproject.toml\", \"Pipfile\", \"Pipfile.lock\",\n",
    "    \"Cargo.toml\", \"Cargo.lock\", \"go.mod\", \"go.sum\", \"pom.xml\",\n",
    "    \"build.gradle\", \"build.gradle.kts\", \"settings.gradle\", \"settings.gradle.kts\",\n",
    "    \"Gemfile\", \"Gemfile.lock\", \"composer.json\", \"composer.lock\",\n",
    "    \"*.csproj\", \"*.fsproj\", \"packages.config\", \"global.json\",\n",
    "    \"conanfile.txt\", \"conanfile.py\", \"vcpkg.json\", \"mix.exs\", \"rebar.config\", \"rebar.lock\",\n",
    "    \"DESCRIPTION\", \"renv.lock\", \"packrat.lock\", \"cpanfile\", \"Makefile.PL\", \"Build.PL\",\n",
    "    \"stack.yaml\", \"cabal.project\", \"*.cabal\", \"dune\", \"dune-project\", \"*.opam\",\n",
    "    \"build.sbt\", \"project/*.scala\", \"Project.toml\", \"Manifest.toml\",\n",
    "    \"pubspec.yaml\", \"nimble.json\", \"v.mod\", \"build.zig.zon\",\n",
    "    \"deno.json\", \"import_map.json\", \"Dockerfile\", \"*.sh\", \"install.sh\",\n",
    "    \"*.tf\", \"terraform.lock.hcl\", \"default.nix\", \"flake.nix\", \"AndroidManifest.xml\",\n",
    "    \"Podfile\", \"Cartfile\", \"Package.swift\", \"project.clj\", \"deps.edn\",\n",
    "    \"requirements.yml\", \"playbook.yml\", \"Chart.yaml\", \"requirements.yaml\"\n",
    "]\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {GITHUB_TOKEN}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Pattern matching\n",
    "exact_matches = set()\n",
    "wildcard_patterns = []\n",
    "\n",
    "for pattern in dependency_files:\n",
    "    if '*' in pattern or '?' in pattern or '/' in pattern:\n",
    "        wildcard_patterns.append(pattern)\n",
    "    else:\n",
    "        exact_matches.add(pattern)\n",
    "\n",
    "def matches_dependency_file(filename):\n",
    "    if filename in exact_matches:\n",
    "        return True\n",
    "    return any(fnmatch.fnmatch(filename, pattern) for pattern in wildcard_patterns)\n",
    "\n",
    "# Retry wrapper for API requests\n",
    "def request_with_backoff(url, headers, max_retries=5):\n",
    "    retry_delay = 1\n",
    "    max_delay = 300\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            res = requests.get(url, headers=headers)\n",
    "            if res.status_code == 403:\n",
    "                reset = int(res.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "                wait_time = max(reset - time.time(), 60)\n",
    "                print(f\"‚è≥ Rate limited. Sleeping for {wait_time:.1f}s...\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            elif res.status_code != 200:\n",
    "                print(f\"‚ùå Error {res.status_code} from {url}\")\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay = min(retry_delay * 2, max_delay)\n",
    "                continue\n",
    "            return res\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Network error: {e}. Retrying in {retry_delay}s...\")\n",
    "            time.sleep(retry_delay)\n",
    "            retry_delay = min(retry_delay * 2, max_delay)\n",
    "    return None\n",
    "\n",
    "def get_repo_tree(owner, repo):\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/git/trees/HEAD?recursive=1\"\n",
    "    res = request_with_backoff(url, HEADERS)\n",
    "    if res:\n",
    "        try:\n",
    "            return res.json().get(\"tree\", [])\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå JSON parsing error in get_repo_tree: {e}\")\n",
    "    return []\n",
    "\n",
    "def get_dependency_file_content(owner, repo, path):\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n",
    "    res = request_with_backoff(url, HEADERS)\n",
    "    if res:\n",
    "        try:\n",
    "            content = res.json()\n",
    "            if content.get(\"encoding\") == \"base64\":\n",
    "                import base64\n",
    "                return base64.b64decode(content[\"content\"]).decode(\"utf-8\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error decoding content for {path}: {e}\")\n",
    "    return None\n",
    "\n",
    "def get_dependencies(owner, repo):\n",
    "    tree = get_repo_tree(owner, repo)\n",
    "    found_deps = []\n",
    "\n",
    "    for item in tree:\n",
    "        path = item.get(\"path\")\n",
    "        if item.get(\"type\") == \"blob\" and matches_dependency_file(path):\n",
    "            found_deps.append(path)\n",
    "\n",
    "    return found_deps\n",
    "\n",
    "def main():\n",
    "    results = []\n",
    "\n",
    "    with open(CSV_PATH, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            full_name = row['FOSS project']\n",
    "            if not full_name or '/' not in full_name:\n",
    "                continue\n",
    "\n",
    "            owner, repo = full_name.strip().split(\"/\", 1)\n",
    "            print(f\"üîç Scanning {owner}/{repo} for dependency files...\")\n",
    "            deps = get_dependencies(owner, repo)\n",
    "\n",
    "            results.append({\n",
    "                \"FOSS project\": full_name,\n",
    "                \"dependency_files\": deps,\n",
    "                \"description\": row.get(\"description\", \"\"),\n",
    "                \"stars\": int(row.get(\"stars\", 0)),\n",
    "                \"forks\": int(row.get(\"forks\", 0)),\n",
    "                \"issues\": int(row.get(\"issues\", 0)),\n",
    "                \"watchers\": int(row.get(\"watchers\", 0)),\n",
    "                \"last updated\": row.get(\"last_updated\", \"\")\n",
    "            })\n",
    "\n",
    "            time.sleep(1)  # Avoid hammering API\n",
    "\n",
    "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ Finished writing data for {len(results)} projects to {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CVE-CPE-FOSS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
