{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          FOSS project  \\\n",
      "0  mtdvio/every-programmer-should-know   \n",
      "1                storybookjs/storybook   \n",
      "2                       facebook/react   \n",
      "3     yangshun/tech-interview-handbook   \n",
      "4                       996icu/996.ICU   \n",
      "\n",
      "                                        dependencies  \n",
      "0                                                 []  \n",
      "1  [package.json, scripts/clean-merged-branches.s...  \n",
      "2  [compiler/apps/playground/scripts/link-compile...  \n",
      "3                     [package.json, pnpm-lock.yaml]  \n",
      "4                                                 []  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "dependency_info_path = Path(\"../enriched_github_data/FOSS_projects_with_dependency_info.json\")\n",
    "\n",
    "# # Initialize an empty list to store the relevant data\n",
    "# relevant_data = []\n",
    "\n",
    "# # Open the large JSON file and process each entry\n",
    "# with open(dependency_info_path, 'r', encoding='utf-8') as f:\n",
    "#     # For each line in the file (assuming it's a JSON array of objects)\n",
    "#     for line in f:\n",
    "#         try:\n",
    "#             entry = json.loads(line.strip())  # Load each JSON entry one by one\n",
    "#             foss_project = entry.get(\"FOSS project\")\n",
    "#             dep_info = entry.get(\"dependency_info\", {})\n",
    "\n",
    "#             if foss_project and dep_info:\n",
    "#                 dependencies = list(dep_info.keys())  # Adjust based on your need\n",
    "#                 relevant_data.append({\n",
    "#                     'FOSS project': foss_project,\n",
    "#                     'dependencies': dependencies\n",
    "#                 })\n",
    "#         except json.JSONDecodeError as e:\n",
    "#             print(f\"Skipping invalid JSON entry: {e}\")\n",
    "\n",
    "# # Convert the relevant_data list into a pandas DataFrame\n",
    "# df = pd.DataFrame(relevant_data)\n",
    "\n",
    "# # Print or check the DataFrame\n",
    "# print(df.head())\n",
    "\n",
    "import ijson\n",
    "import pandas as pd\n",
    "\n",
    "# filename = \"FOSS_projects_with_dependency_info.json\"\n",
    "\n",
    "# Open and stream the file\n",
    "rows = []\n",
    "with open(dependency_info_path, 'r', encoding='utf-8') as f:\n",
    "    objects = ijson.items(f, 'item')  # 'item' targets elements in the top-level array\n",
    "    for obj in objects:\n",
    "        # Optionally filter here for memory efficiency\n",
    "        rows.append({\n",
    "            'FOSS project': obj.get('FOSS project'),\n",
    "            'dependencies': list(obj.get('dependency_info', {}).keys())\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          FOSS project  \\\n",
      "13665  anuraghazra/github-readme-stats   \n",
      "13666            fighting41love/funNLP   \n",
      "13667            elastic/elasticsearch   \n",
      "13668                      vitejs/vite   \n",
      "13669             Snailclimb/JavaGuide   \n",
      "\n",
      "                                            dependencies  \n",
      "13665  [package-lock.json, package.json, scripts/push...  \n",
      "13666                                                 []  \n",
      "13667  [.buildkite/packer_cache.sh, .buildkite/script...  \n",
      "13668  [package.json, pnpm-lock.yaml, scripts/docs-ch...  \n",
      "13669                     [package.json, pnpm-lock.yaml]  \n"
     ]
    }
   ],
   "source": [
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FOSS project': 'storybookjs/storybook', 'dependencies': ['package.json', 'scripts/clean-merged-branches.sh', 'yarn.lock']}\n"
     ]
    }
   ],
   "source": [
    "entry = df[df[\"FOSS project\"] == \"storybookjs/storybook\"]\n",
    "print(entry.to_dict(orient=\"records\")[0])  # Pretty prints the first matching entry as a Python dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['FOSS project', 'dependencies'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import pandas as pd\n",
    "import json\n",
    "rows = []\n",
    "with open(dependency_info_path, 'r', encoding='utf-8') as f:\n",
    "    objects = ijson.items(f, 'item')  # 'item' targets elements in the top-level array\n",
    "    for obj in objects:\n",
    "        project = obj.get('FOSS project')\n",
    "        dep_info = obj.get('dependency_info', {})\n",
    "\n",
    "        # Unescape each file's contents (they're JSON-escaped strings)\n",
    "        unescaped_info = {}\n",
    "        for filename, content in dep_info.items():\n",
    "            try:\n",
    "                unescaped_info[filename] = json.loads(content)\n",
    "            except json.JSONDecodeError:\n",
    "                # If it's not JSON (like .sh or .lock files), keep as raw string\n",
    "                unescaped_info[filename] = content\n",
    "\n",
    "        rows.append({\n",
    "            'FOSS project': project,\n",
    "            'dependency_info': unescaped_info,\n",
    "            'dependency_filenames': list(dep_info.keys())\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          FOSS project  \\\n",
      "0  mtdvio/every-programmer-should-know   \n",
      "1                storybookjs/storybook   \n",
      "2                       facebook/react   \n",
      "3     yangshun/tech-interview-handbook   \n",
      "4                       996icu/996.ICU   \n",
      "\n",
      "                                     dependency_info  \\\n",
      "0                                                 {}   \n",
      "1  {'package.json': {'name': '@storybook/root', '...   \n",
      "2  {'compiler/apps/playground/scripts/link-compil...   \n",
      "3  {'package.json': {'name': 'tech-interview-hand...   \n",
      "4                                                 {}   \n",
      "\n",
      "                                dependency_filenames  \n",
      "0                                                 []  \n",
      "1  [package.json, scripts/clean-merged-branches.s...  \n",
      "2  [compiler/apps/playground/scripts/link-compile...  \n",
      "3                     [package.json, pnpm-lock.yaml]  \n",
      "4                                                 []  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FOSS project': 'storybookjs/storybook', 'dependency_info': {'package.json': {'name': '@storybook/root', 'scripts': {'ci-tests': 'cd code; yarn ci-tests', 'get-report-message': 'cd scripts; yarn get-report-message', 'get-template': 'cd scripts; yarn get-template', 'get-sandbox-dir': 'cd scripts; yarn get-sandbox-dir', 'i': 'yarn --cwd scripts && yarn --cwd code', 'knip': 'cd code; yarn knip', 'lint': 'cd code; yarn lint', 'nx': 'cd code; yarn nx', 'pretty-docs': 'cd scripts; yarn install >/dev/null; yarn docs:prettier:write', 'start': 'yarn task --task dev --template react-vite/default-ts --start-from=install', 'task': \"echo 'Installing Script Dependencies...'; cd scripts; yarn install >/dev/null; cd ..; yarn --cwd=./scripts task\", 'test': 'cd code; yarn test', 'upload-bench': 'cd scripts; yarn upload-bench', 'vite-ecosystem-ci:before-test': 'node ./scripts/vite-ecosystem-ci/before-test.js && cd ./sandbox/react-vite-default-ts && yarn install', 'vite-ecosystem-ci:build': 'yarn task --task sandbox --template react-vite/default-ts --start-from=install', 'vite-ecosystem-ci:test': 'yarn task --task test-runner-dev --template react-vite/default-ts --start-from=dev && yarn task --task test-runner --template react-vite/default-ts --start-from=build && yarn task --task vitest-integration --template react-vite/default-ts --start-from vitest-integration'}, 'packageManager': 'yarn@4.6.0', 'engines': {'node': '>=20.0.0'}}, 'scripts/clean-merged-branches.sh': '#!/bin/sh\\n#/ Usage: clean-merged-branches [-f]\\n#/ Delete merged branches from the origin remote.\\n#/\\n#/ Options:\\n#/   -f            Really delete the branches. Without this branches are shown\\n#/                 but nothing is deleted.\\nset -e\\n\\n# show usage maybe\\n[ \"$1\" = \"--help\" ] && {\\n    grep \\'^#/\\' <\"$0\"| cut -c4-\\n    exit 0\\n}\\n\\n# fetch and prune remote branches\\ngit fetch origin --prune\\n\\n# grab list of merged branches\\nbranches=$(\\n  git branch -a --merged origin/master |\\n  grep remotes/origin/ |\\n  grep -v origin/master |\\n  grep -v \\'enterprise-.*-release\\' |\\n  sed \\'s@remotes/origin/@@\\'\\n)\\n\\n# bail out with no branches\\n[ -z \"$branches\" ] && {\\n    echo \"no merged branches detected\" 1>&2\\n    exit 0\\n}\\n\\n[ \"$1\" != -f ] && {\\n    echo \"These branches will be deleted:\" 1>&2\\n    echo \"$branches\"\\n    read -p \"Press \\'y\\' if you\\'re sure. \" -n 1 -r\\n    echo    # move to a new line\\n}\\n\\n# delete the branches or just show what would be done without -f\\nif [ \"$1\" = -f ] || [[ $REPLY =~ ^[Yy]$ ]]; then\\n    git push origin $(echo \"$branches\" | sed \\'s/^ */:/\\')\\nfi\\n', 'yarn.lock': '# This file is generated by running \"yarn install\" inside your project.\\n# Manual changes might be lost - proceed with caution!\\n\\n__metadata:\\n  version: 8\\n  cacheKey: 10\\n\\n\"@storybook/root@workspace:.\":\\n  version: 0.0.0-use.local\\n  resolution: \"@storybook/root@workspace:.\"\\n  languageName: unknown\\n  linkType: soft\\n'}, 'dependency_filenames': ['package.json', 'scripts/clean-merged-branches.sh', 'yarn.lock']}\n",
      "{'ci-tests': 'cd code; yarn ci-tests', 'get-report-message': 'cd scripts; yarn get-report-message', 'get-template': 'cd scripts; yarn get-template', 'get-sandbox-dir': 'cd scripts; yarn get-sandbox-dir', 'i': 'yarn --cwd scripts && yarn --cwd code', 'knip': 'cd code; yarn knip', 'lint': 'cd code; yarn lint', 'nx': 'cd code; yarn nx', 'pretty-docs': 'cd scripts; yarn install >/dev/null; yarn docs:prettier:write', 'start': 'yarn task --task dev --template react-vite/default-ts --start-from=install', 'task': \"echo 'Installing Script Dependencies...'; cd scripts; yarn install >/dev/null; cd ..; yarn --cwd=./scripts task\", 'test': 'cd code; yarn test', 'upload-bench': 'cd scripts; yarn upload-bench', 'vite-ecosystem-ci:before-test': 'node ./scripts/vite-ecosystem-ci/before-test.js && cd ./sandbox/react-vite-default-ts && yarn install', 'vite-ecosystem-ci:build': 'yarn task --task sandbox --template react-vite/default-ts --start-from=install', 'vite-ecosystem-ci:test': 'yarn task --task test-runner-dev --template react-vite/default-ts --start-from=dev && yarn task --task test-runner --template react-vite/default-ts --start-from=build && yarn task --task vitest-integration --template react-vite/default-ts --start-from vitest-integration'}\n"
     ]
    }
   ],
   "source": [
    "entry = df[df[\"FOSS project\"] == \"storybookjs/storybook\"]\n",
    "print(entry.to_dict(orient=\"records\")[0])  # Pretty prints the first matching entry as a Python dict\n",
    "\n",
    "pkg_json = df[df['FOSS project'] == 'storybookjs/storybook'].iloc[0]['dependency_info']['package.json']\n",
    "print(pkg_json['scripts'])  # prints the scripts object from package.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to JSON format\n",
    "df.to_json(\"flattened_data.json\", orient=\"records\", lines=False,indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSplit completed. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m files created.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Call the function to split the large JSON file\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[43msplit_json_safely\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflattened_data.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msplit_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m, in \u001b[0;36msplit_json_safely\u001b[0;34m(input_file, output_prefix, max_size_mb)\u001b[0m\n\u001b[1;32m     21\u001b[0m current_json \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()  \u001b[38;5;66;03m# Accumulate the line into the current JSON object\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Try parsing the accumulated string as JSON\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_json\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# If parsing succeeds, we have a complete document\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     chunk\u001b[38;5;241m.\u001b[39mappend(doc)\n",
      "File \u001b[0;32m~/miniconda3/envs/pydriller_env/lib/python3.13/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/miniconda3/envs/pydriller_env/lib/python3.13/json/decoder.py:345\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    341\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/miniconda3/envs/pydriller_env/lib/python3.13/json/decoder.py:361\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    358\u001b[0m \n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 361\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pydriller_env/lib/python3.13/json/decoder.py:32\u001b[0m, in \u001b[0;36mJSONDecodeError.__init__\u001b[0;34m(self, msg, doc, pos)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg, doc, pos):\n\u001b[0;32m---> 32\u001b[0m     lineno \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     33\u001b[0m     colno \u001b[38;5;241m=\u001b[39m pos \u001b[38;5;241m-\u001b[39m doc\u001b[38;5;241m.\u001b[39mrfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m, pos)\n\u001b[1;32m     34\u001b[0m     errmsg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: line \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m column \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (char \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (msg, lineno, colno, pos)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def split_json_safely(input_file, output_prefix, max_size_mb=16):\n",
    "    \"\"\"\n",
    "    Safely splits a large JSON array file into smaller files without cutting entries in half.\n",
    "    :param input_file: Path to the large JSON file to be split.\n",
    "    :param output_prefix: Prefix for the output split files.\n",
    "    :param max_size_mb: Maximum size of each output file in MB (default is 16MB).\n",
    "    \"\"\"\n",
    "    # Max size in bytes (default 16MB)\n",
    "    max_size_bytes = max_size_mb * 1024 * 1024\n",
    "    \n",
    "    # Open the large file and start reading line by line\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        chunk = []\n",
    "        chunk_size = 0\n",
    "        chunk_count = 0\n",
    "        current_json = ''\n",
    "        \n",
    "        for line in f:\n",
    "            current_json += line.strip()  # Accumulate the line into the current JSON object\n",
    "            \n",
    "            try:\n",
    "                # Try parsing the accumulated string as JSON\n",
    "                doc = json.loads(current_json)\n",
    "                # If parsing succeeds, we have a complete document\n",
    "                \n",
    "                chunk.append(doc)\n",
    "                chunk_size += len(json.dumps(doc))  # Calculate the approximate size of this document\n",
    "\n",
    "                # If the chunk size exceeds the limit, write to a new file\n",
    "                if chunk_size > max_size_bytes:\n",
    "                    chunk_count += 1\n",
    "                    output_file = f\"{output_prefix}_{chunk_count}.json\"\n",
    "                    with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "                        json.dump(chunk, out_f)\n",
    "                    \n",
    "                    # Reset for the next chunk\n",
    "                    chunk = [doc]\n",
    "                    chunk_size = len(json.dumps(doc))\n",
    "                current_json = ''  # Reset after successfully parsing a complete document\n",
    "            except json.JSONDecodeError:\n",
    "                # If the current string is not a complete JSON object, keep accumulating lines\n",
    "                continue\n",
    "        \n",
    "        # Write any remaining documents in the last chunk\n",
    "        if chunk:\n",
    "            chunk_count += 1\n",
    "            output_file = f\"{output_prefix}_{chunk_count}.json\"\n",
    "            with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "                json.dump(chunk, out_f)\n",
    "        \n",
    "    print(f\"Split completed. {chunk_count} files created.\")\n",
    "\n",
    "# Call the function to split the large JSON file\n",
    "split_json_safely('flattened_data.json', 'split_data')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydriller_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
