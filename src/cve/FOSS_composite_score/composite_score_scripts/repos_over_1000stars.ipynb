{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time \n",
    "import json\n",
    "load_dotenv()\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "HEADERS = {\"Authorization\": f\"Bearer {GITHUB_TOKEN}\"}\n",
    "API_URL = \"https://api.github.com/graphql\"\n",
    "\n",
    "query = \"\"\"\n",
    "query ($after: String) {\n",
    "  search(query: \"stars:>1000\", type: REPOSITORY, first: 50, after: $after) {\n",
    "    repositoryCount\n",
    "    pageInfo {\n",
    "      endCursor\n",
    "      hasNextPage\n",
    "    }\n",
    "    edges {\n",
    "      node {\n",
    "        ... on Repository {\n",
    "          nameWithOwner\n",
    "          description\n",
    "          stargazerCount\n",
    "          forkCount\n",
    "          url\n",
    "          updatedAt\n",
    "          issues(states: OPEN) {\n",
    "            totalCount\n",
    "          }\n",
    "          watchers {\n",
    "            totalCount\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_data = []\n",
    "cursor = None\n",
    "\n",
    "while True:\n",
    "    variables = {\"after\": cursor}\n",
    "    response = requests.post(API_URL, json={\"query\": query, \"variables\": variables}, headers=HEADERS)\n",
    "    data = response.json()\n",
    "    print(data)\n",
    "\n",
    "    # Optional: basic error handling\n",
    "    if \"errors\" in data:\n",
    "        print(\"GraphQL error:\", data[\"errors\"])\n",
    "        break\n",
    "\n",
    "    return_data.append(data)\n",
    "\n",
    "    page_info = data['data']['search']['pageInfo']\n",
    "    if not page_info[\"hasNextPage\"]:\n",
    "        break\n",
    "\n",
    "    cursor = page_info[\"endCursor\"]\n",
    "\n",
    "    # Optional: sleep to respect rate limits\n",
    "    time.sleep(1)\n",
    "\n",
    "# Write all responses to a file\n",
    "with open(\"github_response.json\", \"w\") as f:\n",
    "    json.dump(return_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages to fetch.\n",
      "Fetched 20 pages of data.\n"
     ]
    }
   ],
   "source": [
    "CURSOR_FILE = 'github_cursor.json'\n",
    "DATA_FILE = 'github_response.json'\n",
    "\n",
    "def save_state(cursor, data):\n",
    "    with open(CURSOR_FILE, 'w') as f:\n",
    "        json.dump({'cursor': cursor}, f)\n",
    "    with open(DATA_FILE, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "def load_state():\n",
    "    if os.path.exists(CURSOR_FILE) and os.path.exists(DATA_FILE):\n",
    "        with open(CURSOR_FILE) as f:\n",
    "            cursor = json.load(f).get('cursor')\n",
    "        with open(DATA_FILE) as f:\n",
    "            data = json.load(f)\n",
    "        return cursor, data\n",
    "    return None, []\n",
    "\n",
    "def run_query():\n",
    "    cursor, return_data = load_state()\n",
    "    retry_delay = 1\n",
    "    max_delay = 300\n",
    "\n",
    "    while True:\n",
    "        variables = {'after': cursor}\n",
    "        try:\n",
    "            response = requests.post(API_URL, json={'query': query, 'variables': variables}, headers=HEADERS)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f'Connection error: {e}, retrying in {retry_delay}s...')\n",
    "            time.sleep(retry_delay)\n",
    "            retry_delay = min(retry_delay * 2, max_delay)\n",
    "            continue\n",
    "\n",
    "        if response.status_code == 403:\n",
    "            reset = int(response.headers.get('X-RateLimit-Reset', time.time() + 60))\n",
    "            wait_time = max(reset - time.time(), 60)\n",
    "            print(f'Rate limited! Sleeping for {wait_time:.2f} seconds...')\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f'Error {response.status_code}: {response.text}, retrying in {retry_delay}s...')\n",
    "            time.sleep(retry_delay)\n",
    "            retry_delay = min(retry_delay * 2, max_delay)\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        if 'errors' in data:\n",
    "            print('GraphQL error:', data['errors'])\n",
    "            break\n",
    "\n",
    "        return_data.append(data)\n",
    "        save_state(cursor, return_data)\n",
    "\n",
    "        page_info = data['data']['search']['pageInfo']\n",
    "        if not page_info['hasNextPage']:\n",
    "            print('No more pages to fetch.')\n",
    "            break\n",
    "\n",
    "        cursor = page_info['endCursor']\n",
    "        retry_delay = 1  # reset delay after success\n",
    "        time.sleep(1)    # be nice to GitHub\n",
    "\n",
    "    print(f'Fetched {len(return_data)} pages of data.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_query()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydriller_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
